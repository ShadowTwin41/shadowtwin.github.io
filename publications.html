<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Publications</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">
  <link rel="stylesheet" href="/css/background-pub.css">
  <link rel="stylesheet" href="/css/button.css">
  <link rel="stylesheet" href="/css/text.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

  <style>
    /* --- Global Styles --- */
    html, body {
      margin: 0;
    }

    /* --- Header Styles & Animation --- */
    .header {
      max-height: 300px; /* Adjust if your header is taller */
      overflow: hidden;
      text-align: center;
      transition: max-height 0.6s ease-in-out, padding 0.6s ease-in-out;
    }

    .header-content {
      transition: opacity 0.3s ease;
    }

    body.dropdown-active .header {
      max-height: 0;
      padding-top: 0;
      padding-bottom: 0;
    }

    body.dropdown-active .header-content {
      opacity: 0;
    }

    /* --- Navbar Styles --- */
    .navbar {
      position: -webkit-sticky; /* For Safari */
      position: sticky;
      top: 0;
      z-index: 1000;
      width: 100%;
      display: flex;
      box-sizing: border-box;
      background-color: #33333300;
      font-family: Verdana, sans-serif;
      align-items: stretch;
      flex-wrap: nowrap;
      transition: background-color 0.3s ease;
    }

    body.dropdown-active .navbar {
      background-color: rgba(51, 51, 51, 0.95); /* A slightly transparent background when open */
    }
    
    /* --- Dropdown & Button Styles --- */
    .dropdown {
      position: relative;
      display: flex;
      flex: 1 1 0;
      min-width: 0;
      align-items: stretch;
    }

    .dropdown .dropbtn {
      cursor: pointer;
      font-size: clamp(0.5rem, 4vw, 1.6rem);
      border: none;
      outline: none;
      color: white;
      padding: 16px 16px;
      background-color: inherit;
      font-family: inherit;
      margin: 0;
      font-weight: bold;
      flex: 1 1 0;
      align-items: center;
      justify-content: center;
      box-sizing: border-box;
      white-space: nowrap;
    }
    
    .navbar .dropbtn {
      opacity: 0;
      transform: translateY(20px);
      animation: fadeInUp 0.7s ease forwards;
      will-change: transform, opacity;
    }

    .navbar .dropdown:nth-child(1) .dropbtn { animation-delay: 0.08s; }
    .navbar .dropdown:nth-child(2) .dropbtn { animation-delay: 0.16s; }
    .navbar .dropdown:nth-child(3) .dropbtn { animation-delay: 0.24s; }
    .navbar .dropdown:nth-child(4) .dropbtn { animation-delay: 0.32s; }
    .navbar .dropdown:nth-child(5) .dropbtn { animation-delay: 0.40s; }
    .navbar .dropdown:nth-child(6) .dropbtn { animation-delay: 0.48s; }

    .navbar a:hover, .dropdown:hover .dropbtn, .dropbtn:focus {
      background-color: rgb(0, 0, 83);
    }

    /* --- Dropdown Content Styles --- */
    .dropdown-content {
      display: none;
      position: fixed;
      left: 0;
      right: 0;
      width: auto;
      z-index: 1100;
      overflow-y: auto;
      background-color: #ffffffc7;
      padding: 1rem;
      box-shadow: 0 2px 8px rgba(0,0,0,0.2);
      animation: fadeInUp 0.7s ease forwards;
      opacity: 0;
    }
    
    .show {
      display: block;
    }

    .dropdown-content p { margin: 10px 5%; text-align: left; max-width: 95%; font-size: 0.9rem; line-height: 1.6; }
    .dropdown-content p.keywords { margin: 10px 5%; text-align: left; max-width: 100%; font-size: 0.9rem; line-height: 1.6; }
    .dropdown-content p.keywords::before { content: "Keywords: "; font-weight: bold; font-size: 1.0rem; }
    .first-author-with-border { border-bottom: 0px solid rgb(0, 0, 255); padding: 0; margin: 0 auto; }
    .first-author-with-border h1 { display: block; font-size: 1.6rem; font-weight: bold; color: rgb(0, 0, 255); text-align: center; padding: 10px 0; margin: 0; background-color: rgba(0, 0, 255, 0.05); width: 100%; }
    .first-author-with-borde h2 { display: block; margin: 30px auto 1rem; font-size: 1.2rem; font-weight: bold; color: rgb(0, 0, 83); text-align: center; max-width: 80%; }
    .mid-author-with-border { border-bottom: 2px solid rgb(83, 0, 0); padding: 0; margin: 0 auto; }
    .mid-author-with-border h1 { display: block; font-size: 1.6rem; font-weight: bold; color: rgb(83, 0, 0); text-align: center; padding: 10px 0; margin: 0; background-color: rgba(255, 0, 0, 0.05); width: 100%; box-sizing: border-box; }
    .mid-author-with-border h2 { color: rgb(83, 0, 0); }


    .first-author-with-border .container-flex .text-content h2 {
      color: rgb(0, 0, 83); /* Change link color */
      text-decoration: none; /* Remove underline */
      font-weight: bold; /* Make it bold */
      transition: color 0.3s ease; /* Smooth color change on hover */
    }

    .first-author-with-border .container-flex .text-content h2 a {
      color: rgb(0, 0, 83); /* Change link color */
      text-decoration: none; /* Remove underline */
      font-weight: bold; /* Make it bold */
      transition: color 0.3s ease; /* Smooth color change on hover */
    }

    .first-author-with-border .container-flex .text-content h2 a:hover {
      background-color: #ffffff00;
      color: rgb(0, 0, 154); /* Darker color on hover */
      text-decoration: underline; /* Add underline on hover */
    }

    .first-author-with-border .container-flex .text-content p a:hover {
      background-color: #ffffff00;
      color: rgb(0, 0, 154); /* Darker color on hover */
      text-decoration: underline; /* Add underline on hover */
    }

    .mid-author-with-border .container-flex .text-content h2 a {
      color: rgb(83, 0, 0); /* Change link color */
      text-decoration: none; /* Remove underline */
      font-weight: bold; /* Make it bold */
      transition: color 0.3s ease; /* Smooth color change on hover */
    }

    .mid-author-with-border .container-flex .text-content h2 a:hover {
      background-color: #ffffff00;
      color: rgb(154, 0, 0); /* Darker color on hover */
      text-decoration: underline; /* Add underline on hover */
    }

    .mid-author-with-border .container-flex .text-content p a:hover {
      background-color: #ffffff00;
      color: rgb(0, 0, 154); /* Darker color on hover */
      text-decoration: underline; /* Add underline on hover */
    }
    /* ============================================
    === STYLES FOR THE PUBLICATION BLOCK      ===
    ============================================
    */

  /* Main container (mobile-first) */
  .container-flex {
    display: flex;
    flex-direction: column;
    gap: 1.5rem;
    align-items: center;
  }

  /* Text container (mobile-first) */
  .text-content {
    width: 100%;
  }

  /* Figure container (mobile-first) */
  .image-content {
    width: 100%; 
    max-width: 400px;
    margin: 0;
  }

  .image-content .figure-img {
    width: 100%;
    height: auto;
    display: block;
  }

  /* --- Media Query for Wide Screens (Desktop) --- */
  @media (min-width: 992px) {
    .container-flex {
      flex-direction: row; /* Switch to side-by-side */
      gap: 2rem;
    }

    /* This is the key fix for the 50/50 split */
    .container-flex > .text-content,
    

    .image-content img {
      width: 100%;
      height: 100%;
      object-fit: cover; /* This is the key part */
    }

    /* This fixes the paragraph margin issue */
    .container-flex .text-content h2,
    .container-flex .text-content p {
        margin-inline: 0; /* Remove the extra 5% side margin */
    }
  }

    /* --- [NEW] HOVER AND MODAL STYLES --- */
    .figure-img {
      cursor: pointer;
      transition: transform 0.3s ease, box-shadow 0.3s ease;
    }

    .figure-img:hover {
      transform: scale(1.05);
      box-shadow: 0 4px 15px rgba(0,0,0,0.2);
    }

    .modal-overlay {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background: rgb(211, 211, 211);
      display: flex;
      justify-content: center;
      align-items: center;
      z-index: 2000;
      cursor: pointer;
      opacity: 0;
      transition: opacity 0.3s ease;
    }
    
    .modal-overlay.active {
      opacity: 1;
    }

    .modal-img {
      max-width: 90vw;
      max-height: 90vh;
      object-fit: contain;
      transform: scale(0.9);
      transition: transform 0.3s ease;
    }

    .modal-overlay.active .modal-img {
      transform: scale(1);
    }
    
</style>
  
</head>
<body>

  <header class="header"> 
    <div class="header-content">
      <h1>Publications</h1>
      <p>9 first author publications + 12 mid author publications</p>
    </div>
  </header>

    <button class="house-button" onclick="location.href='/'" aria-label="Homepage">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
          <path d="M3 9.75L12 3l9 6.75V21a.75.75 0 01-.75.75h-5.25v-6h-6v6H3.75A.75.75 0 013 21V9.75z"/>
        </svg>
    </button>

  <main>
    
    <div class="navbar">
      <div class="dropdown">
        <button class="dropbtn" data-target="2025_papers">2025
          <i class="fa fa-caret-down"></i>
        </button>
        <div class="dropdown-content" id="2025_papers">
          <div class="first-author-with-border">
            <h1>First author</h1>
            <div class="container-flex">
              <div class="text-content">
                <h2>Achieving Over 10× Faster Sample Generation with conditional DDPM</h2>
                <p>
                  We present a unified denoising-diffusion framework for BraTS 2025 Task 8 (synthesizing missing MRI modalities) and Task 9 (inpainting pathology-free regions), with a fast inference strategy over 10× quicker than our BraTS 2024 solution while keeping low computational cost. Results — Task 8: Dice 0.86, SSIM 0.77; Task 9: RMSE 0.053, PSNR 26.77, SSIM 0.918. Code to be released soon.
                </p>
                <p class="keywords">Conditional Denoising Diffusion, IDDPM, Synthetic Data Generation, MRI Reconstruction, Computational Efficiency, Image Inpainting</p>
              </div>
              <figure class="figure image-content">
                <img src="/imgs/2025/Brats_2025_task8_9.png" class="figure-img">
              </figure>
            </div>
          </div>
          
          <br>
          <div class="mid-author-with-border">
            <h1>Mid author</h1>
            <br>
            <div class="container-flex">
              <div class="text-content">
                <h2> <a href="https://www.sciencedirect.com/science/article/pii/S2352340925000198"> GBM-Reservoir: Brain tumor (Glioblastoma Multiforme) MRI dataset collection with ground truth segmentation masks </a> </h2>
                <p>
                  We present a brain tumor MRI dataset with 23,049 samples, each including FLAIR, T1, T1ce, and T2 scans, plus one or two segmentation masks. The dataset expands 438 original BraTS 2022 cases through registration, generating additional samples with varied tumor locations. Despite heterogeneous imaging protocols, it provides ground truth for each sample, supporting the development of deep learning–based automated tumor segmentation on unseen cases.
                </p>
                <p class="keywords">Brain tumor segmentation, Data augmentation, Registration, BraTS, Deep learning</p>
              </div>
              <figure class="figure image-content">
                <img src="/imgs/2025/GBM-Reservoir.png" class="figure-img">
              </figure>
            </div>
            <hr>
            <div class="container-flex">
              <div class="text-content">
                <h2><a href="https://www.medrxiv.org/content/10.1101/2025.06.11.25329022v1">Beyond Benchmarks: Towards Robust Artificial Intelligence Bone Segmentation in Socio-Technical Systems</a></h2>
                <p>
                  We evaluated 20 state-of-the-art mandibular segmentation models on 19,218 segmentations from 1,000 multicenter CT/CBCT scans and found accuracy can vary by up to 25% due to factors like voxel size, bone orientation, and patient conditions. While smaller isotropic voxels and neutral orientation improved performance, metallic osteosynthesis and complex anatomy degraded it. These results highlight that AI models are not “plug-and-play” and provide evidence-based recommendations to improve clinical integration.
                </p>
                <p class="keywords">Artificial Intelligence, Image Processing, Computer-Assisted Imaging, Three-Dimensional, 50 Tomography, X-Ray Computed, Practice Guideline, Mandible</p>
              </div>
              <figure class="figure image-content">
                <img src="/imgs/2025/Beyond_Benchmarks.png" class="figure-img">
              </figure>
            </div>
          </div>
        </div>
      </div> 
      <div class="dropdown">
        <button class="dropbtn" data-target="2024_papers">2024
          <i class="fa fa-caret-down"></i>
        </button>
        <div class="dropdown-content" id="2024_papers">
          <div class="first-author-with-border">
            <h1>First author</h1>
            <div class="container-flex">
              <div class="text-content">
                <h2> <a href="https://arxiv.org/abs/2411.04632v1?utm_source=tldrai">Improved Segmentation with Synthetic Data</a></h2>
                <p>
                  We present our winning Task 1 and third-place Task 3 solutions for BraTS, using synthetic data to train SOTA segmentation pipelines—improving robustness for post-treatment glioma segmentation though the pipeline was less suited to meningioma.
                  Results — Task 1 (ET, NETC, RC, SNFH, TC, WT): DSC = 0.790, 0.808, 0.776, 0.893, 0.787, 0.894; HD95 = 35.63, 30.35, 44.58, 16.87, 38.19, 17.95. Task 3 (test): DSC = 0.801, HD95 = 38.26. Code available on <a href="https://github.com/ShadowTwin41/BraTS_2023_2024_solutions">BraTS_2023_2024_solutions</a>.
                </p>
                <p class="keywords">Brain Tumour Segmentation, Synthetic data, nnUnet, MedNeXt, Swin-UNETR</p>
              </div>
              <figure class="figure image-content">
                <img src="/imgs/2024/BraTS_2024_task1_3.png" class="figure-img">
              </figure>
            </div>
            <hr>
            <div class="container-flex">
              <div class="text-content">
                <h2> <a href="https://arxiv.org/abs/2411.04630">Brain Tumour Removing and Missing Modality Generation using 3D WDM</a></h2>
                <p>
                  We present our second-place solution for BraTS 2024 Task 8 (and a participation entry for Task 7) that uses conditional 3D wavelet diffusion models to handle lesions and missing MRI modalities. By applying a wavelet transform we train and infer at full resolution on a 48 GB GPU without patching or downsampling—preserving all image information and improving prediction reliability; code is available on <a href="https://github.com/ShadowTwin41/BraTS_2023_2024_solutions">BraTS_2023_2024_solutions</a>.
                </p>
                <p class="keywords">3D WDM, MRI, Brain Tumour, Inpainting, Missing Modality</p>
              </div>
              <figure class="figure image-content">
                <img src="/imgs/2024/BraTS_2024_task7_8.png" class="figure-img">
              </figure>
            </div>
            <hr>
            <div class="container-flex">
              <div class="text-content">
                <h2><a href="https://arxiv.org/abs/2402.17317">How we won BraTS 2023 Adult Glioma challenge? Just faking it! Enhanced Synthetic Data Augmentation and Model Ensemble for brain tumour segmentation</a></h2>
                <p>
                  We address limited medical data for BraTS 2023 Task 1 by using GANs and registration to augment training samples for three models: nnU-Net, Swin UNETR, and the BraTS 2021 winning solution. Combining convolutional and transformer architectures enhances performance. On the validation set, our best model achieves Dice = 0.901, 0.867, 0.851 and HD95 = 14.94, 14.47, 17.70 for whole tumor, tumor core, and enhancing tumor, respectively. Code is available on <a href="https://github.com/ShadowTwin41/BraTS_2023_2024_solutions">BraTS_2023_2024_solutions</a>.
                </p>
                <p class="keywords">Generative adversarial networks, Registration, Synthetic data, Brain Tumour segmentation, nnU-Net</p>
              </div>
              <figure class="figure image-content">
                <img src="/imgs/2024/BraTS_2023.png" class="figure-img">
              </figure>
            </div>
            <hr>
            <div class="container-flex">
              <div class="text-content">
                <h2><a href="https://ieeexplore.ieee.org/document/10635839">Generalisation of segmentation using Generative Adversarial Networks</a></h2>
                <p>
                  We tackle robustness in BraTS 2024 GoAT by using conditional GANs to generate realistic new cases and train a segmentation model that combines convolutions with attention mechanisms. On the validation set we achieve DSC = 0.855 (enhancing tumor), 0.863 (tumor core), 0.883 (whole tumor), and HD95 = 24.83, 24.10, 21.72, respectively.
                </p>
                <p class="keywords">Generative Adversarial Networks, Synthetic data, nnU-Net, Swin UNETR, Segmentation </p>
              </div>
              <figure class="figure image-content">
                <img src="/imgs/2024/BraTS_2024_Goat.png" class="figure-img">
              </figure>
            </div>
            <hr>
            <div class="container-flex">
              <div class="text-content">
                <h2><a href="https://www.sciencedirect.com/science/article/pii/S1361841524000252">GAN-based generation of realistic 3D volumetric data: A systematic review and taxonomy</a></h2>
                <p>
                  We review GAN-based methods for generating realistic volumetric (3D) data—primarily in medicine—motivated by limited datasets caused by rarity, privacy, and high acquisition cost. We summarize common architectures, loss functions and evaluation metrics, present a novel taxonomy, compare advantages and drawbacks, and outline evaluations, open challenges, and research opportunities to give a holistic overview of volumetric GANs.
                </p>
                <p class="keywords">Synthetic volumetric data, Generative adversarial network, Systematic review, Volumetric GANs taxonomy</p>
              </div>
              <figure class="figure image-content">
                <img src="/imgs/2024/GANs_review.png" class="figure-img">
              </figure>
            </div>
            <br>
          </div>
          <br>
          <div class="mid-author-with-border">
            <h1>Mid author</h1>
            <br>
            <div class="container-flex">
              <div class="text-content">
                <h2><a href="https://link.springer.com/chapter/10.1007/978-3-031-83274-1_10">Comparative Analysis of nnUNet and MedNeXt for Head and Neck Tumor Segmentation in MRI-Guided Radiotherapy</a></h2>
                <p>
                  We address limited medical data for BraTS 2023 Task 1 by using GANs and registration to massively augment training samples for three models: nnU-Net, Swin UNETR, and the BraTS 2021 winning solution. Combining convolutional and transformer architectures improves complementarity. Results on the test set — lesion-wise Dice: 0.885, 0.872, 0.869; lesion-wise HD95: 22.84, 22.97, 16.71 (WT, TC, ET). Code available on <a href="https://github.com/NikooMoradi/HNTSMRG24_team_TUMOR">HNTSMRG24_team_TUMOR</a>.
                </p>
                <p class="keywords">HNTS-MRG24, MICCAI24, nnUNet, MedNeXt</p>
              </div>
              <figure class="figure image-content">
                <img src="/imgs/2024/HNTS-MRG24.png" class="figure-img">
              </figure>
            </div>
            <hr>
            <div class="container-flex">
              <div class="text-content">
                <h2><a href="https://arxiv.org/abs/2407.01318">Deep Dive Into MRI: Exploring Deep Learning Applications in 0.55T and 7T MRI</a></h2>
                <p>
                  We review the integration of deep learning into emerging 0.55T and 7T MRI technologies, highlighting how DL enhances image detail, tissue characterization, and diagnostic performance. The paper discusses current applications, benefits, and future directions for combining deep learning with advanced MRI to further improve medical imaging.
                </p>
                <p class="keywords">Magnetic Resonance Imaging, 0.55T MRI, 7T MRI, Deep Learning</p>
              </div>
              <figure class="figure image-content">
                <img src="/imgs/2024/Deep-dive_MRI.png" class="figure-img">
              </figure>
            </div>
            <hr>
            <div class="container-flex">
              <div class="text-content">
                <h2><a href="https://link.springer.com/article/10.1007/s00784-024-05781-5">Insights into Predicting Tooth Extraction from Panoramic Dental Images: Artificial Intelligence vs. Dentists</a></h2>
                <p>
                  We trained a ResNet50 model on 26,956 cropped teeth images from panoramic radiographs to predict whether a tooth should be extracted or preserved. The AI achieved ROC-AUC = 0.901 and PR-AUC = 0.749, outperforming dentists (ROC-AUC = 0.797, PR-AUC = 0.589). These results show that AI can support clinical decision-making by improving accuracy and reducing errors in tooth extraction indications.
                </p>
                <p class="keywords">Tooth Extraction, Surgery, Oral Dentistry, Decision Support Techniques, Deep Learning, Artificial Intelligence</p>
              </div>
              <figure class="figure image-content">
                <img src="/imgs/2024/panoramic_tooth.png" class="figure-img">
              </figure>
            </div>
            <hr>
            <div class="container-flex">
              <div class="text-content">
                <h2><a href="https://ieeexplore.ieee.org/document/10635337">A Baseline Solution for the ISBI 2024 Dreaming Challenge</a></h2>
                <p>
                  We introduce the DREAMING challenge, which explores Diminished Reality for medicine by removing surgical tools from operation scenes to enhance visibility. A new dataset of simulated surgeries with instrument and hand occlusions is presented, and a baseline using an existing video inpainting model is evaluated. Results show promising performance under some conditions but highlight key limitations for real-world surgical use.
                </p>
                <p class="keywords">Deep Learning, Inpainting, Diminished Reality, Surgery</p>
              </div>
              <figure class="figure image-content">
                <img src="/imgs/2024/Baseline_Dreaming_Challenge.png" class="figure-img">
              </figure>
            </div>
            <hr>
            <div class="container-flex">
              <div class="text-content">
                <h2><a href="https://arxiv.org/abs/2402.04301">Deep PCCT: Photon Counting Computed Tomography Deep Learning Applications Review</a></h2>
                <p>
                  This review explores recent advances and applications of Photon Counting Computed Tomography (PCCT) in pre-clinical research, highlighting its ability to overcome traditional imaging limitations like low resolution and noise. It analyzes scanner features, deep learning integration, and radiomic applications, showing PCCT’s promise for improved diagnostic precision while outlining current challenges and future research directions in medical imaging.
                </p>
                <p class="keywords">Photon Counting Computed Tomography, Deep Learning, Radiomics</p>
              </div>
              <figure class="figure image-content">
                <img src="/imgs/2024/Deep_PCCT.png" class="figure-img">
              </figure>
            </div>
          </div>
        </div>
      </div> 
    
      <div class="dropdown">
        <button class="dropbtn" data-target="2023_papers">2023
          <i class="fa fa-caret-down"></i>
        </button>
        <div class="dropdown-content" id="2023_papers">
          <div class="first-author-with-border">
            <h1>First author</h1>
            <div class="container-flex">
              <div class="text-content">
                <h2><a href="https://link.springer.com/chapter/10.1007/978-3-031-76163-8_8">Enhanced Data Augmentation using Synthetic Data for Brain Tumour Segmentation</a></h2>
                <p>
                  We address limited medical data for BraTS 2023 Task 1 by using GANs and registration to massively augment training samples for three models: nnU-Net, Swin UNETR, and the BraTS 2021 winning solution. Combining convolutional and transformer architectures improves complementarity. Results on the test set — lesion-wise Dice: 0.885, 0.872, 0.869; lesion-wise HD95: 22.84, 22.97, 16.71 (WT, TC, ET). Code available on  <a href="https://github.com/ShadowTwin41/BraTS_2023_2024_solutions"> BraTS_2023_2024_solutions</a>.
                </p>
                <p class="keywords">Generative adversarial networks, Registration, Synthetic data, Brain Tumour segmentation, nnU-Net</p>
              </div>
              <figure class="figure image-content">
                <img src="/imgs/2024/BraTS_2023.png" class="figure-img">
              </figure>
            </div>
          </div>
          <br>
          <div class="mid-author-with-border">
            <h1>Mid author</h1>
            <br>
            <div class="container-flex">
              <div class="text-content">
                <h2><a href="https://arxiv.org/abs/2308.16139">MedShapeNet – a large-scale dataset of 3D medical shapes for computer vision</a></h2>
                <p>
                  We introduce MedShapeNet, a large-scale collection of over 100,000 annotated 3D medical shapes derived from real patient imaging, covering anatomy and surgical instruments. Designed to bridge computer vision and medical imaging, it supports tasks such as tumor classification, skull reconstruction, anatomy completion, education, and 3D printing. The data is openly accessible via a web and Python interface, enabling research in discriminative, reconstructive, and variational benchmarks, as well as extended reality applications. The project page is: <a href="https://medshapenet.ikim.nrw/">https://medshapenet.ikim.nrw/</a>.
                </p>
                <p class="keywords">3D medical shapes, benchmark, anatomy education, shapeomics, augmented reality, virtual reality</p>
              </div>
              <figure class="figure image-content">
                <img src="/imgs/2023/medshapenet.webp" class="figure-img">
              </figure>
            </div>
            <hr>
            <div class="container-flex">
              <div class="text-content">
                <h2><a href="https://www.sciencedirect.com/science/article/pii/S2352711023001280">Open-source skull reconstruction with MONAI</a></h2>
                <p>
                  We propose an autoencoder-based deep learning model for cranial and facial defect reconstruction within the MONAI framework, pre-trained on the MUG500+ and SkullFix datasets. The work emphasizes open-sourcing, providing accessible code and pre-trained weights via <a href="https://github.com/Project-MONAI/research-contributions/tree/master/SkullRec/"> MONAI’s official repository </a>. This contribution introduces a pre-trained reconstruction model and demonstrates how MONAI tutorials can be easily adapted for new medical imaging applications like skull reconstruction. 
                </p>
                <p class="keywords">Skull reconstruction, Research contribution, MONAI, Open-source, API, PyTorch, Python, Deep learning, Pre-trained model, Cranial implant design, Cranioplasty, Craniotomy, Craniectomy, CT, BoneHeadFace</p>
              </div>
              <figure class="figure image-content">
                <img src="/imgs/2023/Open-source-skull.png" class="figure-img">
              </figure>
            </div>
            <hr>
            <div class="container-flex">
              <div class="text-content">
                <h2><a href="https://link.springer.com/chapter/10.1007/978-3-031-45642-8_29">Generation of Synthetic X-Rays Images of Rib Fractures Using a 2D Enhanced Alpha-GAN for Data Augmentation</a></h2>
                <p>
                  We developed a generative model to synthesize X-ray images with subtle rib fractures, addressing the scarcity of public datasets limited by ethical and bureaucratic constraints. Although evaluated with quantitative metrics and a Turing test, the generated images lacked sufficient realism due to high dataset heterogeneity, highlighting challenges in modeling fine fracture details.
                </p>
                <p class="keywords">alpha-GAN, data augmentation, X-ray, fractures, rib</p>
              </div>
              <figure class="figure image-content">
                <img src="/imgs/2023/synthetic-x-rays.jpg" class="figure-img">
              </figure>
            </div>
          </div>
        </div>
      </div> 

      <div class="dropdown">
        <button class="dropbtn" data-target="2022_papers">2022
          <i class="fa fa-caret-down"></i>
        </button>
        <div class="dropdown-content" id="2022_papers">
          <div class="first-author-with-border">
            <h1>First author</h1>
            <div class="container-flex">
              <div class="text-content">
                <h2><a href="https://www.mdpi.com/2076-3417/12/10/4844">Generation of Synthetic Rat Brain MRI Scans with a 3D Enhanced Alpha Generative Adversarial Network</a></h2>
                <p>
                  We adapt an α-GAN to generate realistic 3D rat-brain MRIs to overcome scanner scarcity, long acquisition times, and data/privacy constraints. To our knowledge this is the first GAN-based generation of rat MRI; with a new normalization layer and loss terms, we validate realism via quantitative metrics, a Turing test, and a segmentation task — and show that training with our synthetic scans improves segmentation more than conventional augmentation.
                </p>
                <p class="keywords">Alpha Generative Adversarial Network, Data Augmentation, Synthetic Data, MRI rat brain</p>
              </div>
              <figure class="figure image-content">
                <img src="/imgs/2022/SigmaRat.png" class="figure-img">
              </figure>
            </div>
            <hr>
            <div class="container-flex">
              <div class="text-content">
                <h2><a href="https://www.igi-global.com/chapter/generation-of-synthetic-data/301776">Generation of Synthetic Data: A Generative Adversarial Networks Approach</a></h2>
                <p>
                  We adapt an α-GAN to generate unlimited synthetic 3D rat-brain MRIs as a low-cost way to overcome scarce, unrepresentative medical datasets that traditional augmentation cannot solve. We validate realism by visual assessment and demonstrate practical value by improving performance in a segmentation test.
                </p>
                <p class="keywords">Alpha Generative Adversarial Network, Data Augmentation, Deep Learning Models, 3D MRI data sets of rat brain</p>
              </div>
              <figure class="figure image-content">
                <img src="/imgs/2022/IGI-SigmaRat.png" class="figure-img">
              </figure>
            </div>
          </div>
          <br>
          <div class="mid-author-with-border">
            <h1>Mid author</h1>
            <br>
            <div class="container-flex">
              <div class="text-content">
                <h2><a href="https://www.mdpi.com/2075-4418/12/11/2733">Radiomics in Head and Neck Cancer Outcome Predictions</a></h2>
                <p>
                  We use radiomics and machine learning to predict head and neck cancer prognosis—including locoregional recurrences, distant metastases, and overall survival—by extracting features from CT images combined with patient clinical data. Using XGBoost, our models achieved AUC = 0.74 (LR), 0.84 (DM), 0.91 (OS), demonstrating that CT-based radiomic features can effectively support prognosis and clinical decision-making.
                </p>
                <p class="keywords">precision medicine, head and neck cancer, radiomics, locoregional recurrences, distant metastases, overall survival, CT, multilayer perceptron, XGBoost</p>
              </div>
              <figure class="figure image-content">
                <img src="/imgs/2022/radiomics_survival.jpg" class="figure-img">
              </figure>
            </div>
            <hr>
            <div class="container-flex">
              <div class="text-content">
                <h2><a href="https://arxiv.org/abs/2209.01112">AutoPET Challenge: Combining nn-Unet with Swin UNETR Augmented by Maximum Intensity Projection Classifier</a></h2>
                <p>
                  We present an ensemble solution for the AutoPET challenge, combining nnU-Net and Swin UNETR with a maximum intensity projection classifier that gates lesion detection, followed by late fusion of segmentations. Tested on FDG-PET/CT scans from 900 patients, our approach achieves an average cross-validation Dice score of 72.12% for lung cancer, melanoma, and lymphoma. Code is available on <a href="https://github.com/heiligerl/AutoPET_Challenge_Submission">AutoPET_Challenge_Submission</a>.
                </p>
                <p class="keywords">Semantic Segmentation, FDG-PET/CT, Swin UNETR, nn-Unet, Late Fusion</p>
              </div>
              <figure class="figure image-content">
                <img src="/imgs/2022/AutoPET.png" class="figure-img">
              </figure>
            </div>
          </div>
        </div>
      </div> 
    </div>
    
  
  </main>

  <canvas id="particle-canvas"></canvas>

  
  <script src="/js/mask-wave-mouse.js"></script>
  <script src="/js/responsive.js"></script>
  <script src="/js/moving-content.js"></script>
  <script>
  document.addEventListener('DOMContentLoaded', () => {
    const body = document.body;
    const navbar = document.querySelector('.navbar');

    function closeAllDropdowns() {
      document.querySelectorAll('.dropdown-content.show').forEach(dropdown => {
        dropdown.classList.remove('show');
      });
      document.querySelectorAll('.dropbtn').forEach(btn => btn.setAttribute('aria-expanded', 'false'));
      body.classList.remove('dropdown-active');
    }

    function positionDropdown(dropdownElement) {
      if (!dropdownElement) return;
      const navBarHeight = navbar.offsetHeight;
      const viewportHeight = window.innerHeight;
      const availableHeight = viewportHeight - navBarHeight - 20;

      dropdownElement.style.position = 'fixed';
      dropdownElement.style.top = `${navBarHeight}px`;
      dropdownElement.style.left = '0';
      dropdownElement.style.right = '0';
      dropdownElement.style.maxHeight = `${availableHeight}px`;
      dropdownElement.style.overflowY = 'auto';
    }

    document.querySelectorAll('.dropbtn[data-target]').forEach(button => {
      button.addEventListener('click', (event) => {
        event.stopPropagation();
        const targetId = button.dataset.target;
        const targetElement = document.getElementById(targetId);
        if (!targetElement) return;

        const wasAlreadyOpen = targetElement.classList.contains('show');
        closeAllDropdowns();

        if (!wasAlreadyOpen) {
          body.classList.add('dropdown-active');
          const btn = targetElement.parentElement.querySelector('.dropbtn');
          if (btn) btn.setAttribute('aria-expanded', 'true');
          
          setTimeout(() => {
            positionDropdown(targetElement);
            targetElement.classList.add('show');
          }, 50); // This delay is critical for iOS
        }
      });
    });

    document.addEventListener('click', (event) => {
      if (!event.target.closest('.dropdown')) {
        closeAllDropdowns();
      }
    });

    document.addEventListener('keydown', (e) => {
      if (e.key === 'Escape') {
        closeAllDropdowns();
      }
    });

    window.addEventListener('resize', () => {
        const openDropdown = document.querySelector('.dropdown-content.show');
        if (openDropdown) {
            positionDropdown(openDropdown);
        }
    });

    // --- IMAGE MODAL LOGIC (WITH FIX) ---
    const images = document.querySelectorAll('.figure-img');

    images.forEach(image => {
      image.addEventListener('click', () => {
        const overlay = document.createElement('div');
        overlay.classList.add('modal-overlay');

        const modalImage = document.createElement('img');
        modalImage.src = image.src;
        modalImage.classList.add('modal-img');

        overlay.appendChild(modalImage);
        document.body.appendChild(overlay);
        
        setTimeout(() => {
          overlay.classList.add('active');
        }, 10);

        // Add a click listener to the overlay to remove it
        overlay.addEventListener('click', (event) => {
          // **THIS IS THE FIX**: Stop the click from reaching the document listener
          event.stopPropagation(); 
          
          overlay.classList.remove('active');
          
          overlay.addEventListener('transitionend', () => {
            if (document.body.contains(overlay)) {
              document.body.removeChild(overlay);
            }
          }, { once: true });
        });
      });
    });
  });
</script>

</body>
</html>