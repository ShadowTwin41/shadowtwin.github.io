<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Publications</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">
  <link rel="stylesheet" href="/css/background.css">
  <link rel="stylesheet" href="/css/button.css">
  <link rel="stylesheet" href="/css/text.css">
  <style>
    
  </style>

</head>
<body>
  <!-- particle canvas -->
  <canvas id="particle-canvas" aria-hidden="true"></canvas>
  <!-- single responsive image -->
  <div class="bg-wrap" aria-hidden="true">
    <img class="bg-image" src="/imgs/background.jpg" alt="">
  </div>

  <button class="house-button" onclick="location.href='/'" aria-label="Homepage">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
      <path d="M3 9.75L12 3l9 6.75V21a.75.75 0 01-.75.75h-5.25v-6h-6v6H3.75A.75.75 0 013 21V9.75z"/>
    </svg>
  </button>

  <main>
    <!-- FIRST AUTHOR 2025 papers -->
    <div class="publication-class">
      <hr> 
      <h1>2025</h1>

      <h2>Achieving Over 10Ã— Faster Sample Generation with conditional DDPM</h2>
      <p>
        This paper presents our solutions for Task 8 and 9 of BraTS 2025, which respectively involve the generation of a missing MRI modality and inpainting a missing region. Task 8 aims to solve the problem of missing a MRI modality for cases which acquisition is infeasible or the quality is not good enough for analysis. Task 9 seeks to produce pathology-free cases which would allow the analysis of healthy brains.
        We use denoising diffusion models to solve both tasks in a unified framework. Since regular diffusion models require 1000 steps or more for inference, we propose a solution to speed up inference while obtaining competitive results while keeping a low computational footprint. Compared to our solution from BraTS 2024, our new solution achieves better results with over 10 times faster processing.
        We achieve a Dice score of 0.86 and an SSIM of 0.77 on Task 8, and an MSE of 0.005, PSNR of 24.53, and SSIM of 0.868 on the Task 9 online validation set. The code will be made available after the challenge.
      </p>
      <p class="keywords">Conditional Denoising Diffusion, IDDPM, Synthetic Data Generation, MRI Reconstruction, Computational Efficiency, Image Inpainting</p>

      <hr class="hr2">

      <!-- Another paper -->
      <h2>Advancing the Open Suturing Skills Challenge 2025 with Pre-Trained Networks</h2>
      <p>For Task 1, the pipeline is composed of three steps: First, frames are selected using a non-linear sampling strategy that prioritises the final, most informative segments of the procedure. Next, each frame is transformed into a 4-channel input tensor, augmenting the standard RGB data with a Sobel edge-detection channel to enhance fine-grained details of instruments and sutures. Finally, this sequence of feature-enhanced frames is processed by a fine-tuned InceptionV3 architecture to classify overall surgical performance into one of four skill categories.</p>
      <p>In Task 2, the pipeline is composed of two steps: frame preprocessing and skill classification. Surgical videos are sampled using a non-linear strategy that emphasises the final portion of the procedure, and <em>ROIs</em> are extracted with a frozen pre-trained YOLOv5 detector. The resulting sequences are encoded by a <em>ResNet50</em> backbone and modelled with a 3-layer LSTM to capture temporal dynamics. Finally, eight classification heads assign <em>OSATS</em> scores across five skill levels.</p>
      <p>For Task 3, the pipeline is composed of three steps: First, the frames are selected and a fine-tuned SAM2 is used to segment everything in the image; Next, a trained XGBoost is used to classify the masks into an object of interest or not; Lastly, a U-Net is used to predict the keypoints.</p>
      <p>For Task 1, we achieved an F1 score of 0.8869. In Task 2, the results show high performance for class 0 but a sharp drop across other classes, with zero performance in class 4. This indicates overfitting and poor generalisation to under-represented classes, and for Task 3, 0.2241 (HOTA).</p>
      <p class="keywords">Surgery, Skill Evaluation, Tracking, Radiomics, SAM2</p>

      <hr class="hr2">
    </div>

    <!-- 2024 papers -->
    <div class="publication-class">
      <hr> 
      <h1>2024</h1>
      <!-- Brats goat -->
      <h2>Generalisation of segmentation using Generative Adversarial Networks</h2>
      <p>
        State-of-the-art deep learning algorithms are easily biased and evaluated in misleading scenarios, especially in the medical context, where scenarios change rapidly and diseases develop quickly. The BraTS 2024 GoAT challenge aims to evaluate how brain tumour segmentation algorithms can adapt to different circumstances when these are not available for training. Our solution utilises state-of-the-art conditional generative adversarial networks to generate realistic new cases and train a segmentation algorithm that takes advantage of the convolutions and attention mechanisms. Our solution achieved a DSC value of 0.855, 0.863, 0.883 and an HD95 value of 24.83, 24.10 and 21.72 for the enhancing tumour, the tumour core and the whole tumour in the validation set, respectively.
      </p>
      <p class="keywords">Generative Adversarial Networks, Synthetic data, nnU-Net, Swin UNETR, Segmentation </p>
      <hr class="hr2">
      
      <!-- Brats 2024 Task 1 and 3 -->
      <h2>Improved Segmentation with Synthetic Data</h2>
      <p>
        This paper presents the winning solution of task 1 and the third-placed solution of task 3 of the BraTS challenge. The use of automated tools in clinical practice has increased due to the development of more and more sophisticated and reliable algorithms. However, achieving clinical standards and developing tools for real-life scenarios is a major challenge. To this end, BraTS has organised tasks to find the most advanced solutions for specific purposes. In this paper, we propose the use of synthetic data to train state-of-the-art frameworks in order to improve the segmentation of adult gliomas in a post-treatment scenario, and the segmentation of meningioma for radiotherapy planning. Our results suggest that the use of synthetic data leads to more robust algorithms, although the synthetic data generation pipeline is not directly suited to the meningioma task. In task 1, we achieved a DSC of 0.7900, 0.8076, 0.7760, 0.8926, 0.7874, 0.8938 and a HD95 of 35.63, 30.35, 44.58, 16.87, 38.19, 17.95 for ET, NETC, RC, SNFH, TC and WT, respectively and, in task 3, we achieved a DSC of 0.801 and HD95 of 38.26, in the testing phase. The code for these tasks is available at  <a href="https://github.com/ShadowTwin41/BraTS_2023_2024_solutions"> BraTS_2023_2024_solutions</a>.
      </p>
      <p class="keywords">Brain Tumour Segmentation, Synthetic data, nnUnet, MedNeXt, Swin-UNETR</p>
      <hr class="hr2">

      <!-- Brats 2024 task 7 and 8 -->
      <h2>Brain Tumour Removing and Missing Modality Generation using 3D WDM</h2>
      <p>
        This paper presents the second-placed solution for task 8 and the participation solution for task 7 of BraTS 2024. The adoption of automated brain analysis algorithms to support clinical practice is increasing. However, many of these algorithms struggle with the presence of brain lesions or the absence of certain MRI modalities. The alterations in the brain's morphology leads to high variability and thus poor performance of predictive models that were trained only on healthy brains. The lack of information that is usually provided by some of the missing MRI modalities also reduces the reliability of the prediction models trained with all modalities. In order to improve the performance of these models, we propose the use of conditional 3D wavelet diffusion models. The wavelet transform enabled full-resolution image training and prediction on a GPU with 48 GB VRAM, without patching or downsampling, preserving all information for prediction. The code for these tasks is available at <a href="https://github.com/ShadowTwin41/BraTS_2023_2024_solutions"> BraTS_2023_2024_solutions</a>.
      </p>
      <p class="keywords">3D WDM, MRI, Brain Tumour, Inpainting, Missing Modality</p>
      <hr class="hr2">
    </div>

    <!-- 2022 Papers -->
    <div class="publication-class">
      <!-- Rat brains -->
       <hr>
      <h1>2022</h1>

      <h2>Generation of Synthetic Rat Brain MRI Scans with a 3D Enhanced Alpha Generative Adversarial Network</h2>
      <p>
        Translational brain research using Magnetic Resonance Imaging (MRI) is becoming increasingly popular as animal models are an essential part of scientific studies and more ultra-high-field scanners are becoming available. Some disadvantages of MRI are the availability of MRI scanners and the time required for a full scanning session. Privacy laws and the 3Rs ethics rule also make it difficult to create large datasets for training deep learning models. To overcome these challenges, an adaptation of the alpha Generative Adversarial Networks (GANs) architecture was used to test its ability to generate realistic 3D MRI scans of the rat brain in silico.  As far as the authors are aware, this was the first time a GAN-based approach was used to generate synthetic MRI data of the rat brain. The generated scans were evaluated using various quantitative metrics, a Turing test, and a segmentation test. The last two tests proved the realism and applicability of the generated scans to real problems. Therefore, by using the proposed new normalisation layer and loss functions, it was possible to improve the realism of the generated rat MRI scans and it was shown that using the generated data improved the segmentation model more than using the conventional data augmentation.
      </p>
      <p class="keywords">Alpha Generative Adversarial Network, Data Augmentation, Synthetic Data, MRI rat brain</p>
      <hr class="hr2">

    </div>



      <!-- MIDDLE AUTHOR papers -->
    <div class="publication-class">
      <hr> 
      <h2>Advancing the Open Suturing Skills Challenge 2025 with Pre-Trained Networks</h2>
      <p>For Task 1, the pipeline is composed of three steps: First, frames are selected using a non-linear sampling strategy that prioritises the final, most informative segments of the procedure. Next, each frame is transformed into a 4-channel input tensor, augmenting the standard RGB data with a Sobel edge-detection channel to enhance fine-grained details of instruments and sutures. Finally, this sequence of feature-enhanced frames is processed by a fine-tuned InceptionV3 architecture to classify overall surgical performance into one of four skill categories.</p>
      <p>In Task 2, the pipeline is composed of two steps: frame preprocessing and skill classification. Surgical videos are sampled using a non-linear strategy that emphasises the final portion of the procedure, and <em>ROIs</em> are extracted with a frozen pre-trained YOLOv5 detector. The resulting sequences are encoded by a <em>ResNet50</em> backbone and modelled with a 3-layer LSTM to capture temporal dynamics. Finally, eight classification heads assign <em>OSATS</em> scores across five skill levels.</p>
      <p>For Task 3, the pipeline is composed of three steps: First, the frames are selected and a fine-tuned SAM2 is used to segment everything in the image; Next, a trained XGBoost is used to classify the masks into an object of interest or not; Lastly, a U-Net is used to predict the keypoints.</p>
      <p>For Task 1, we achieved an F1 score of 0.8869. In Task 2, the results show high performance for class 0 but a sharp drop across other classes, with zero performance in class 4. This indicates overfitting and poor generalisation to under-represented classes, and for Task 3, 0.2241 (HOTA).</p>
      <p class="keywords">Surgery, Skill Evaluation, Tracking, Radiomics, SAM2</p>

      <hr class="hr2">
    </div>

    
    

   

  </main>

  <!-- JS scripts -->
  <script src="/js/mask-wave-mouse.js"></script>
  <script src="/js/particles.js"></script>
  <script src="/js/responsive.js"></script>
</body>
</html>
